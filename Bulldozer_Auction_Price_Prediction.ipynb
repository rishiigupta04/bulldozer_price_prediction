{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CV4kLw7NhEeA"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/mrdbourke/zero-to-mastery-ml/raw/master/data/bluebook-for-bulldozers.zip # download files from GitHub as zip\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = 'bluebook-for-bulldozers.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "\n",
        "zip_ref.extractall('.') # extract all data into current working directory\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "dnruhHqfhTcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/bluebook-for-bulldozers/TrainAndValid.csv\")"
      ],
      "metadata": {
        "id": "XjlsnHp_hfVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(3)"
      ],
      "metadata": {
        "id": "1gwGBuIbhzrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "Fs75HUeSiBZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "ifq0cHJZjz2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "CHxFRSi3kW00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.scatter(df[\"saledate\"][:1000], df[\"SalePrice\"][:1000])"
      ],
      "metadata": {
        "id": "ef3Jbz0rkmZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"saledate\"][:1000]"
      ],
      "metadata": {
        "id": "Af6N3Hcqk4Yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"SalePrice\"].plot.hist()"
      ],
      "metadata": {
        "id": "gIzxzHsflQ3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Since we are dealing with time series dataset, we need to organize the dates as much as possible, we can do that by telling pandas which of our columns has dates in it using the 'parse_dates' parameter\n"
      ],
      "metadata": {
        "id": "uZ3klxwzmBpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import data again but this time parse dates\n",
        "\n",
        "df = pd.read_csv(\"/content/bluebook-for-bulldozers/TrainAndValid.csv\", parse_dates=[\"saledate\"])"
      ],
      "metadata": {
        "id": "IOt_MBZMmBZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.saledate"
      ],
      "metadata": {
        "id": "E1LZ0yZwmaWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.scatter(df[\"saledate\"][:1000], df[\"SalePrice\"][:1000])\n",
        "\n",
        "#Now the index is properly organzied according to Year on X axis"
      ],
      "metadata": {
        "id": "tZjm7K5AmyzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head().T"
      ],
      "metadata": {
        "id": "cNGbHM3Xm_eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head().T"
      ],
      "metadata": {
        "id": "lujbCgwrn5H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.saledate.head(20)"
      ],
      "metadata": {
        "id": "CjTcD-VVoROB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## When working with time series data, it's a good practice to sort the data by date"
      ],
      "metadata": {
        "id": "eRtfpGEFosWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort_values(by=[\"saledate\"], inplace=True, ascending=True)"
      ],
      "metadata": {
        "id": "foByYN4mosAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.saledate.head(20)"
      ],
      "metadata": {
        "id": "-3jhDMFKoc62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "jwfaZPbypA3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp = df.copy()"
      ],
      "metadata": {
        "id": "ZQdaqSQIpXrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add datetime parameters for `saledate` column"
      ],
      "metadata": {
        "id": "ZZR2RdJ_q66d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp[\"saleYear\"] = df_tmp.saledate.dt.year\n",
        "df_tmp[\"saleMonth\"] = df_tmp.saledate.dt.month\n",
        "df_tmp[\"saleDay\"]= df_tmp.saledate.dt.day\n",
        "df_tmp[\"saleDayOfWeek\"] = df_tmp.saledate.dt.dayofweek\n",
        "df_tmp[\"saleDayOfYear\"] = df_tmp.saledate.dt.dayofyear"
      ],
      "metadata": {
        "id": "qvMCoK7Ppcjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp.head().T"
      ],
      "metadata": {
        "id": "rLBQ_C2hsQ77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp.drop(columns=[\"saledate\"], inplace=True)"
      ],
      "metadata": {
        "id": "cHw96EzosSfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp.state.value_counts()"
      ],
      "metadata": {
        "id": "IHNfqNB1shiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp.head().T"
      ],
      "metadata": {
        "id": "3gMLMiICtank"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now let's do some model-drive EDA\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "44OkazPfuJM_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##- Handling Missing Values/Duplicates\n",
        "##- Feature Transformation\n",
        "##- Feature Encoding\n",
        "##- Converting Categorical cols to Numerical\n",
        "##- Feature Scaling (if required)"
      ],
      "metadata": {
        "id": "nqSBxdyFvH9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp.info()"
      ],
      "metadata": {
        "id": "Vb4Ule0LvElM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Select only the columns that have the 'object' dtype\n",
        "#    .select_dtypes() returns a new DataFrame containing only those columns\n",
        "df_categorical_subset = df_tmp.select_dtypes(include=['object'])\n",
        "\n",
        "# 2. Extract the column names as a list\n",
        "categorical_column_names = df_categorical_subset.columns\n",
        "\n",
        "# 3. Convert only those specific columns in the original DataFrame in place\n",
        "#    by applying .astype('category') to the selection\n",
        "df_tmp[categorical_column_names] = df_tmp[categorical_column_names].astype('category')"
      ],
      "metadata": {
        "id": "2NodwQ1lv7vD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp.info()"
      ],
      "metadata": {
        "id": "LxuRTlZbyIeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Once data is stored using the pandas category data type, we have a way to access that data in its underlying numeric format.\n",
        "\n",
        "- The category dtype is designed specifically to optimize storage by mapping string labels to a compact, integer-based representation internally.\n",
        "\n",
        "- You can access these underlying integer values using the .cat.codes attribute of the pandas Series:"
      ],
      "metadata": {
        "id": "0MERW-m-1Jrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp.state.cat.codes"
      ],
      "metadata": {
        "id": "g2r6KAikyTgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Save preprocessed data\n",
        "\n",
        "df_tmp.to_csv(\"preprocessed_data.csv\", index=False)"
      ],
      "metadata": {
        "id": "EtFrU1_I2N-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "kwz1f4b02Sfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Identify and deal with missing values\n",
        "\n",
        "df_tmp.isnull().sum()/len(df_tmp)"
      ],
      "metadata": {
        "id": "mvXDAfsZ1UPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp.isnull().sum()"
      ],
      "metadata": {
        "id": "vlzZnhWs1f2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Fill numeric missing values first\n",
        "\n",
        "for label, content in df_tmp.items():\n",
        "  if pd.api.types.is_numeric_dtype(content):\n",
        "    print(label)"
      ],
      "metadata": {
        "id": "-zklAC2R198b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for which numeric cols have null values\n",
        "\n",
        "for label, content in df_tmp.items():\n",
        "  if pd.api.types.is_numeric_dtype(content):\n",
        "    if pd.isnull(content).sum():\n",
        "      print(label)"
      ],
      "metadata": {
        "id": "bc93JP-r3BKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill them with median\n",
        "\n",
        "for label, content in df_tmp.items():\n",
        "  if pd.api.types.is_numeric_dtype(content):\n",
        "    if pd.isnull(content).sum():\n",
        "      #Add a binary column which tells us if the data was missing\n",
        "      df_tmp[label+\"_is_missing\"] = pd.isnull(content)\n",
        "\n",
        "      df_tmp[label] = content.fillna(content.median())"
      ],
      "metadata": {
        "id": "dRCQPsqZ3dVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check to see how many examples were missing\n",
        "\n",
        "df_tmp.auctioneerID_is_missing.value_counts()"
      ],
      "metadata": {
        "id": "NWlrvFHV4J47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp.isnull().sum()"
      ],
      "metadata": {
        "id": "4VzYSz_m4exd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#Filling and turning categorical variables into numbers"
      ],
      "metadata": {
        "id": "pt3c769r5QP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for label, content in df_tmp.items():\n",
        "  if not pd.api.types.is_numeric_dtype(content):\n",
        "    print(label)"
      ],
      "metadata": {
        "id": "N8OWDMT-5CgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " for label, content in df_tmp.items():\n",
        "  if not pd.api.types.is_numeric_dtype(content):\n",
        "      #Add a binary column which tells us if the sample had a missing value\n",
        "      df_tmp[label+\"_is_missing\"] = pd.isnull(content)\n",
        "\n",
        "      #Turn categories into numbers and add +1 since pandas assigns -1 to values that are missing\n",
        "      df_tmp[label] = pd.Categorical(content).codes+1"
      ],
      "metadata": {
        "id": "miYcN22j5cb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp.info()"
      ],
      "metadata": {
        "id": "6Bbs6s9M9pEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp.head().T[:50]"
      ],
      "metadata": {
        "id": "M4RLkoyy95zT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp.isnull().sum()"
      ],
      "metadata": {
        "id": "mO0TZMfp-KM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tmp[\"UsageBand\"].value_counts()"
      ],
      "metadata": {
        "id": "JB07MN9p_D1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FX4CAH4V-Z6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Building"
      ],
      "metadata": {
        "id": "doCcChag_lRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_tmp)"
      ],
      "metadata": {
        "id": "rYoBDU9I-YX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X = df_tmp.drop(columns=[\"SalePrice\"],axis =1)\n",
        "# y = df_tmp[\"SalePrice\"]"
      ],
      "metadata": {
        "id": "ANXZvpsz_31f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "\n",
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# model = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
        "\n",
        "# model.fit(X, y)"
      ],
      "metadata": {
        "id": "XcZv4tgB_rho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.score(X,y) #leads to Overfitting, model memorizes the test data"
      ],
      "metadata": {
        "id": "xzoyW4zZAzyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Splitting the data into training and validation sets"
      ],
      "metadata": {
        "id": "Iqu2O_YlJJTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this is how we usually do it in time series data\n",
        "\n",
        "df_val = df_tmp[df_tmp['saleYear'] >= 2012]\n",
        "df_train = df_tmp[df_tmp['saleYear'] != 2012]\n",
        "\n",
        "len(df_train), len(df_val)"
      ],
      "metadata": {
        "id": "6_okNfKCJEDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split data into X and y\n",
        "\n",
        "X_train, y_train = df_train.drop(\"SalePrice\",axis=1), df_train[\"SalePrice\"]\n",
        "X_val, y_val = df_val.drop(\"SalePrice\",axis=1), df_val[\"SalePrice\"]"
      ],
      "metadata": {
        "id": "m-NDmMcdJ1LE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating our own Evaluation Function"
      ],
      "metadata": {
        "id": "7IOuzHgxTiu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import root_mean_squared_log_error,mean_squared_log_error, mean_absolute_error,r2_score\n",
        "\n",
        "def show_scores(model):\n",
        "  train_preds = model.predict(X_train)\n",
        "  val_preds = model.predict(X_val)\n",
        "\n",
        "  scores = {\"Training MAE\": mean_absolute_error(y_train, train_preds),\n",
        "            \"Validation MAE\": mean_absolute_error(y_val, val_preds),\n",
        "            \"Training RMSLE\": root_mean_squared_log_error(y_train, train_preds),\n",
        "            \"Validation RMSLE\": root_mean_squared_log_error(y_val, val_preds),\n",
        "            \"Training R2 Score\": r2_score(y_train, train_preds),\n",
        "            \"Validation R2 Score\": r2_score(y_val, val_preds)\n",
        "            }\n",
        "  return scores"
      ],
      "metadata": {
        "id": "Sm_2C8eOK1Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing our model on a subset (to tune the hyperparameters)"
      ],
      "metadata": {
        "id": "R7kPrZhQX7DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "model = RandomForestRegressor(n_jobs=-1, random_state=42, max_samples=10000)\n",
        "\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "hNffxbQhUJBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_scores(model)"
      ],
      "metadata": {
        "id": "M3xdAYqjY8AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hyperparameter Tuning with RandomizedSearchCV"
      ],
      "metadata": {
        "id": "v5rye6cuaKHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "#Different RandomForestRegressor hyperparameters\n",
        "\n",
        "rf_grid = {\n",
        "    \"n_estimators\": np.arange(10, 100, 10),\n",
        "    \"max_depth\": [None,3,5,10],\n",
        "    \"min_samples_split\": np.arange(2,20,2),\n",
        "    \"min_samples_leaf\": np.arange(1,20,2),\n",
        "    \"max_features\": [0.5,1,\"sqrt\",\"auto\"],\n",
        "    \"max_samples\": [10000]\n",
        "}\n",
        "\n",
        "rs_model = RandomizedSearchCV(model,\n",
        "                              param_distributions=rf_grid,\n",
        "                              n_iter=5,\n",
        "                              cv = 5,\n",
        "                              verbose=True)\n",
        "\n",
        "rs_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "kJPJCpyRZIPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train a model with the best hyperparameters\n",
        "\n",
        "- found after 100 iterations of `RandomizedSearchCV`"
      ],
      "metadata": {
        "id": "zP1SyN8PdNYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "ideal_model = RandomForestRegressor(n_jobs=-1, random_state=42,\n",
        "                                    min_samples_split=14,\n",
        "                                    min_samples_leaf=1,\n",
        "                                    n_estimators=40,\n",
        "                                    max_features=0.5,\n",
        "                                    max_samples = None)\n",
        "\n",
        "ideal_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "2DaqhgifdhNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_scores(rs_model) #trained on only 10,000 samples"
      ],
      "metadata": {
        "id": "rZydpjqhb010"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_scores(ideal_model)#trained on the whole training dataset"
      ],
      "metadata": {
        "id": "4urC-O9lcv31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb6f7af3"
      },
      "source": [
        "import joblib\n",
        "\n",
        "# Save the trained model to a file using joblib\n",
        "joblib.dump(ideal_model, 'ideal_model.joblib')\n",
        "\n",
        "print(\"Model saved successfully as 'ideal_model.joblib'.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make predictions on the test data"
      ],
      "metadata": {
        "id": "ooawiOLOhUrb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess the data (getting the test dataset in the same format as the training dataset)"
      ],
      "metadata": {
        "id": "Fm8JgZu7jCnF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv(\"/content/bluebook-for-bulldozers/Test.csv\", parse_dates=[\"saledate\"])\n",
        "df_test.head()"
      ],
      "metadata": {
        "id": "Bn0z-HbWk6O-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cb495cd"
      },
      "source": [
        "def preprocess_data(df):\n",
        "  \"\"\"\n",
        "  Performs transformations on df and returns the transformed df.\n",
        "  \"\"\"\n",
        "  df[\"saleYear\"] = df.saledate.dt.year\n",
        "  df[\"saleMonth\"] = df.saledate.dt.month\n",
        "  df[\"saleDay\"] = df.saledate.dt.day\n",
        "  df[\"saleDayOfWeek\"] = df.saledate.dt.dayofweek\n",
        "  df[\"saleDayOfYear\"] = df.saledate.dt.dayofyear\n",
        "\n",
        "  df.drop(\"saledate\", axis=1, inplace=True)\n",
        "\n",
        "  # Fill the numeric rows with median and handle categorical columns\n",
        "  for label, content in df.items():\n",
        "    if pd.api.types.is_numeric_dtype(content):\n",
        "      if pd.isnull(content).sum():\n",
        "        # Add a binary column which tells us if the data was missing\n",
        "        df[label+\"_is_missing\"] = pd.isnull(content)\n",
        "        # Replace the nan with the median\n",
        "        df[label] = content.fillna(content.median())\n",
        "    else:\n",
        "      # For categorical columns, add a binary column for missing values\n",
        "      df[label+\"_is_missing\"] = pd.isnull(content)\n",
        "      # Turn categories into numbers and add +1 since pandas assigns -1 to values that are missing\n",
        "      df[label] = pd.Categorical(content).codes+1\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55152506"
      },
      "source": [
        "df_test = preprocess_data(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48110d70"
      },
      "source": [
        "df_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_in_test_cols = set(X_train.columns) - set(df_test.columns)\n",
        "\n",
        "for col in missing_in_test_cols:\n",
        "    if col.endswith(\"_is_missing\"):\n",
        "        df_test[col] = False\n",
        "    else:\n",
        "        df_test[col] = 0\n",
        "\n",
        "extra_in_test_cols = set(df_test.columns) - set(X_train.columns)\n",
        "df_test.drop(columns=list(extra_in_test_cols), inplace=True)\n",
        "\n",
        "df_test = df_test[X_train.columns]"
      ],
      "metadata": {
        "id": "eb2_nGCyqXc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ddfa315"
      },
      "source": [
        "test_preds = ideal_model.predict(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Converting it into a dataframe in a format which Kaggle is asking for"
      ],
      "metadata": {
        "id": "hD_2YYArrkBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_preds = pd.DataFrame()\n",
        "df_preds[\"SalesID\"] = df_test[\"SalesID\"]\n",
        "df_preds[\"SalePrice\"] = test_preds"
      ],
      "metadata": {
        "id": "A9q2BdQBrvEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_preds"
      ],
      "metadata": {
        "id": "38mPo_3Er554"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export prediction data to csv to be submiited on Kaggle\n",
        "\n",
        "df_preds.to_csv(\"test_predictions.csv\", index = False)"
      ],
      "metadata": {
        "id": "57uFEuB6r6ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dcfac8b"
      },
      "source": [
        "import joblib\n",
        "\n",
        "# Save the X_train columns to a file\n",
        "joblib.dump(X_train.columns, 'X_train_columns.joblib')\n",
        "\n",
        "print(\"X_train column names saved successfully as 'X_train_columns.joblib'.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find Feature Importance of our best Model\n"
      ],
      "metadata": {
        "id": "oySAGDSJtWxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9GxRAU1JtoEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4466086d"
      },
      "source": [
        "# Task\n",
        "Create a Streamlit application named `app.py`. This application should:\n",
        "1. Load the `ideal_model.joblib` and `X_train_columns.joblib` files.\n",
        "2. Define a `preprocess_data` function that transforms input data similarly to how the training data was processed.\n",
        "3. Identify the top 5 most important features from the loaded model's feature importances.\n",
        "4. Create a user interface using Streamlit to collect input for these top 5 features.\n",
        "5. Process the user input, transform it using `preprocess_data`, and make a prediction using the loaded model.\n",
        "6. Display the predicted 'SalePrice' to the user.\n",
        "\n",
        "Finally, provide the complete `app.py` code and instructions on how to run it, assuming `ideal_model.joblib` and `X_train_columns.joblib` are in the same directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9020d698"
      },
      "source": [
        "## Load Model and Feature Names\n",
        "\n",
        "### Subtask:\n",
        "Load the `ideal_model.joblib` and `X_train_columns.joblib` files within the `app.py` script. This will be crucial for making predictions and ensuring feature consistency.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b6b4237"
      },
      "source": [
        "**Reasoning**:\n",
        "To load the saved model and feature names, I need to use the `joblib` library to deserialize the files. I will import joblib and then use `joblib.load()` for each file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3068544d"
      },
      "source": [
        "import joblib\n",
        "\n",
        "# Load the trained machine learning model\n",
        "ideal_model = joblib.load('ideal_model.joblib')\n",
        "\n",
        "# Load the list of training column names\n",
        "X_train_columns = joblib.load('X_train_columns.joblib')\n",
        "\n",
        "print(\"Model and feature columns loaded successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e00bd10d"
      },
      "source": [
        "**Reasoning**:\n",
        "To proceed with the Streamlit application and meet the requirement of identifying the top 5 most important features, I need to access the `feature_importances_` attribute of the loaded `ideal_model` and associate them with their respective column names from `X_train_columns`. This will allow me to rank and select the most influential features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48f2b92c"
      },
      "source": [
        "feature_importances = ideal_model.feature_importances_\n",
        "feature_names = X_train_columns\n",
        "\n",
        "# Create a Series for easier sorting and identification\n",
        "importance_df = pd.Series(feature_importances, index=feature_names)\n",
        "\n",
        "# Get the top 5 most important features\n",
        "top_5_features = importance_df.nlargest(5)\n",
        "\n",
        "print(\"Top 5 most important features:\")\n",
        "print(top_5_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f3bd8be"
      },
      "source": [
        "**Reasoning**:\n",
        "To ensure that new input data is processed in the same way as the training data, I will recreate the `preprocess_data` function within the `app.py` script. This function will handle date feature engineering, fill numeric missing values with medians (and create `_is_missing` columns), and convert categorical features to numerical codes (also with `_is_missing` columns).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7f77c96"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_data(df, X_train_columns):\n",
        "    \"\"\"\n",
        "    Performs transformations on df to match the training data format.\n",
        "    Assumes df has a 'saledate' column and needs feature engineering and missing value imputation.\n",
        "    \"\"\"\n",
        "\n",
        "    # Feature engineering for saledate\n",
        "    df[\"saleYear\"] = df.saledate.dt.year\n",
        "    df[\"saleMonth\"] = df.saledate.dt.month\n",
        "    df[\"saleDay\"] = df.saledate.dt.day\n",
        "    df[\"saleDayOfWeek\"] = df.saledate.dt.dayofweek\n",
        "    df[\"saleDayOfYear\"] = df.saledate.dt.dayofyear\n",
        "    df.drop(\"saledate\", axis=1, inplace=True)\n",
        "\n",
        "    # Fill numeric missing values and create _is_missing columns\n",
        "    for label, content in df.items():\n",
        "        if pd.api.types.is_numeric_dtype(content):\n",
        "            if pd.isnull(content).sum():\n",
        "                df[label+\"_is_missing\"] = pd.isnull(content)\n",
        "                df[label] = content.fillna(content.median())\n",
        "        else:\n",
        "            # For categorical columns, add a binary column for missing values\n",
        "            df[label+\"_is_missing\"] = pd.isnull(content)\n",
        "            # Turn categories into numbers and add +1 since pandas assigns -1 to values that are missing\n",
        "            df[label] = pd.Categorical(content).codes+1\n",
        "\n",
        "    # Align columns with training data\n",
        "    missing_in_test_cols = set(X_train_columns) - set(df.columns)\n",
        "    for col in missing_in_test_cols:\n",
        "        if col.endswith(\"_is_missing\"):\n",
        "            df[col] = False\n",
        "        else:\n",
        "            df[col] = 0\n",
        "\n",
        "    extra_in_test_cols = set(df.columns) - set(X_train_columns)\n",
        "    df.drop(columns=list(extra_in_test_cols), inplace=True)\n",
        "\n",
        "    # Ensure the order of columns matches X_train_columns\n",
        "    df = df[X_train_columns]\n",
        "\n",
        "    return df\n",
        "\n",
        "print(\"Preprocess data function defined.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}